\documentclass[a4paper,11pt]{article}

\usepackage{url}
\usepackage{paralist}
\usepackage{authblk}
\usepackage[pdftex,colorlinks=true,hyperfootnotes=false]{hyperref}

\begin{document}

\title{We Need to Talk About Reproducibility in Computing Science...}

\author[1]{Tom Crick}
\author[2]{Benjamin A. Hall}
\author[3]{Samin Ishtiaq}
\affil[1]{Department of Computing, Cardiff Metropolitan University, UK}
\affil[2]{University of Cambridge, UK}
\affil[3]{Microsoft Research, Cambridge, UK}
%\affil[ ]{\protect\url{{tcrick}@cardiffmet.ac.uk}}

% \author{
% Tom Crick\\
% {\url{tcrick@cardiffmet.ac.uk}}\\
% Department of Computing\\
% Cardiff Metropolitan University, UK
% \and
% Ben Hall\\
% {\url{bh418@mrc-cu.cam.ac.uk}}\\
% ???\\

% \and
% Samin Ishtiaq\\
% {\url{sishtiaq@microsoft.com}}\\

% }

\date{ }
\maketitle

\begin{abstract}
Reproducibility is a key tenet of all modern sciences.  Unfortunately
the current state of reproducibility in computational sciences is
lamentable.  We propose that journals in computational sciences should
encourage papers reproducing (or failing to reproduce) earlier work in
their field, by establishing clear criteria for assessing
reproducibility papers.
\end{abstract}


% \section*{Deadlines}
% \begin{compactdesc}
% \item[15 Nov:] Digital Science Catalyst Grant
% \item[15 Dec:] Microsoft Azure for Research Initiative Grant
% \item[30 Dec:] Send CACM paper to Moshe
% \item[30 Jan:] CAV 2015
% \end{compactdesc}

\section*{Previous papers on which to build...}
\begin{compactdesc}
\item[WSSSPE2:] Accepted and presented in Nov~\cite{crick-et-al_wssspe2};
\item[Recomputability 2014:] Accepted and presented in Dec~\cite{crick-et-al_recomp2014};
\item[CAV 2015:] Rejected, but template for CS
  conference~\cite{crick-et-al-cav};
\item[JORS:] Submitted -- updated invited submission from WSSSPE2
  paper~\cite{crick-et-al:2015};
\item[IEEE eScience 2015:] Submitted -- tweaked and generalised CAV
  paper~\cite{crick-et-al_escience};
\item[Irreprodubibility:] 1st April fun~\cite{chuehong-et-al:2015}.
\end{compactdesc}

\section{Intro}


Samin and I, together with Tom Crick (Cardiff) have written some
conference papers around the ideas of reproducibility in science and specifically in
scientific software development. Following our first paper, we have
been asked to write a contributed article for CACM and we were
planning on using it as an opportunity to present a prototype software
platform for compiling and testing software on the cloud, whilst it's
being edited by different parties. The general idea is that initially
a user checks their latest code into visual studio online or
github. This triggers an event on a cloud server, causing the new
version to be pulled to the server, external dependencies downloaded,
the code to be compiled, and several tests run on the new
binaries. One plan for the prototype is to have the BMA compile and be
tested using this mechanism, as a proof of concept.


After the last submission, I have been thinking about the relationship
between the two papers. I was glad to have written the second one as I
felt it was really the second half of the first; one said that
implementation was hard, whereas the second was focused on how models
themselves might limit reproducibity efforts. I would like this
manuscript to bridge both of these topics, and particularly I'd like
to address the CI system aspects. So, roughly I'd hope that the paper
would read in 3 sections:

 \begin{itemize}
\item Implementation problems. Expand to include broader topics in the
literature (e.g. the machine learning reference I sent around)

\item Models/benchmarks. To be frank, I'd like to be more direct
addressing performance based benchmarks here than we were in the
previous paper. Whilst I realize this is hard on the cloud, the more I
have thought about it I don't think counting operations (as suggested)
makes much sense- plus Samin and I have an example from a recent paper
(Samin- our CAV paper) where counting operations on a small benchmark
set wrongly indicated efficiency.

\item The system itself. I don't know what your thoughts are, but I
wonder if it would be possible to set up a simple piece of software,
with a couple of dependencies, just to show what it looked like. We
just decide on an analysis (I can suggest some simple ones), write a
small piece of code which does it (with a couple of dependencies), and
make the code test itself during the build process. One specific
example of a tool that might be appropriate could be a protein
structure analysis called the Gaussian Network Model. Its easy to
implement (and I've done it a few times), and could be linked to the
PDB for a set of tests/benchmarks.
\end{itemize}



\section{Tom Intro}

\subsection{Bullet Points}
Not sure about the name "reproducibility papers" because "reproducible
papers" is a name for, e.g., Sweave'd papers.

We should look through journal instructions to authors for mention of
reproducibility both on output (ie. new papers should ahve some nod
towards reproducibility) and more importantly on input (are
reproducibility studies explicitly encouraged.)  Probably best to
archive them and put the details online, and put data in the
paper. Should be sensible and reasonable sized sample (e.g. all ACM
journals, journals in some other sensible ranking).

\begin{itemize}


\item initiatives in other disciplines (Cardiff person?) pre-registration?
\item Positive results may be unpublishable!   

\item Reproducibility

\item Culture Change 
\item Incentives.  

\item Feeling a fraud as open stuff... true across the field.  E.g. we trust resutls from others and reproduce things.

\item cf dissemination model is broken... impending crisis

\item E.g. give examples of ourselves.   ICLP 09?  Ian can find lots, honest!

\item Examples of tracks applications ... 

\item part of core phd skills training
\item instead of giving a student 100 papers to read and write a literature review, give them one and write a reproducibility studies
\item thinking of Ian Miguel and Lars Kotthoff, got journal papers out of lit reviews
\item Ian's first paper about counterfactuals





\item Example of good journals
\begin{itemize}
\item Springer?
\item IPOL
\item
Journal 2010-2012
Never published a paper!
Folded into Code 
http://www.scfbm.org/content/pdf/1751-0473-7-2.pdf

\item reproducibility project in psychology?

\item thing where people reproduced old papers.  (repo 

\end{itemize}



\end{itemize}

\subsection{ Review standards }

Make a particular proposal. 

Reproducibility is broad ... 


Quality of work and paper is AIJ quality
Novelty not assessed in normal way
Originality is in study of reproducibility of some major piece of AI
Could expose flaws in conventional wisdom if original experiments not reproducible
Could show that conventional wisdom is completely correct 
Either way community has solid basis for understanding 
Often expect new critical insights e.g. on importance of neglected implementation issues
Paper itself should have very high standard of reproducibility
Compare survey paper:
Novelty not assessed in normal way
Likely to improve understanding through selection, presentation, and new insights



\section{Infrastructure}

Techstack. 

github API, VSO. 


Definition: what is the thing we are building. Can be written without actually building it. 

Description: What we built using github. 


\section{What's next?}

Post this to lots of people

% acknowledgements?

\bibliographystyle{abbrv}
\bibliography{reproducibilityCACM}

\end{document}
